This project investigates how adversarial attacks and data poisoning can affect the performance and security of ML models used in wireless communication systems. Using MATLAB, we simulate a 5 Hz wireless signal and introduce different types of attacks to evaluate how models like Support Vector Machines (SVM) and simple Neural Networks handle these threats. Our results show that SVMs are more precise but less adaptive to noise, while neural networks show better recall and resilience to poisoned data. We include both the SVM and Neural Network scripts (FinalSVMpresi.mlx and FinalNNpresi.mlx), as well as the final report and presentation. The project also discusses key recommendations such as adversarial training, noise-resistant algorithms, differential privacy in federated systems, and the use of reinforcement learning for adaptive anomaly detection. This work is intended to raise awareness of the security risks in ML-integrated wireless systems and propose practical solutions for improving their robustness.

